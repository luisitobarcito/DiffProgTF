{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning  Boolean functions by example.\n",
    "Even though this might look like an overkill, it will be helpful to explain the concept of learning by example and the modular perspective. Let's say we want to learn two basic functions: the AND, and the OR functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a generic tunable function.\n",
    "Our generic parameterized function module will model binary input to binary output vectors. Rather than producing binary digits, the generic function can output a real number between o and 1. These outputs can be interpreted as the probability of the output being 1, which also correspond to the expected value of the output under the Bernoulli distribution. Our function corresponds to a affine function followed by a squashing nonlinearlity such the logistic sigmoid.\n",
    "\\begin{equation}\n",
    "f(x) = \\sigma(Wx + b)\n",
    "\\end{equation}\n",
    "where $\\sigma(z) = 1/(1+e^{-z})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenFunc(object):\n",
    "    def __init__(self, n_in, n_out=1, act_func=None):\n",
    "        self.W = tf.Variable(tf.random_normal([n_in, n_out], stddev=0.35), dtype=tf.float32)\n",
    "        self.b = tf.Variable(tf.zeros([n_out], dtype=tf.float32), dtype=tf.float32)\n",
    "        if act_func is None:\n",
    "            self.act_func = tf.sigmoid\n",
    "        else:\n",
    "            self.act_func = act_func\n",
    "\n",
    "    def __call__(self, X, is_logit=False):\n",
    "        # For numerical stability, We can select logit output \n",
    "        # to train the function with cross-entropy \n",
    "        if is_logit is False:\n",
    "            return self.act_func(tf.add(tf.matmul(X, self.W), self.b))\n",
    "        else:\n",
    "            return tf.add(tf.matmul(X, self.W), self.b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a differentiable objective\n",
    "We use cross entropy function to evaluate the match of the output produced by our tunable function and the desired output of our program. In the context of binary digits and our tunable function representing the probability of being 1, the cross entropy function is also a likelihood function of the parameters given the desired input output data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learnProgram(sess, f, train_in, train_out, n_iter=10000):\n",
    "    X = tf.placeholder(shape=train_in.shape, dtype=tf.float32)\n",
    "    Y = tf.placeholder(shape=train_out.shape, dtype=tf.float32)      \n",
    "    ## The cross entropy loss applied to the function to be tuned\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=f(X, is_logit=True)))\n",
    "    ## Training procedure\n",
    "    trainer = tf.train.GradientDescentOptimizer(learning_rate=0.2)\n",
    "    train_step = trainer.minimize(loss)\n",
    "    tf.global_variables_initializer().run()\n",
    "    for iTr in range(n_iter):\n",
    "        sess.run(train_step, feed_dict={X: train_in, Y: train_out})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The OR function\n",
    "The data for the OR function is given by the following truth table:\n",
    "\n",
    "| X1 | X2 | X1 OR X2 |\n",
    "|----|:--:|:--------:|\n",
    "| 0  | 0  |    0     |\n",
    "| 0  | 1  |    1     |\n",
    "| 1  | 0  |    1     |\n",
    "| 1  | 1  |    1     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X1    X2    X1 OR X2\n",
      "----  ----  ----------\n",
      "   0     0           0\n",
      "   0     1           1\n",
      "   1     0           1\n",
      "   1     1           1\n"
     ]
    }
   ],
   "source": [
    "## Create our set of input output pairs\n",
    "X = np.asarray([[0, 0],[0, 1],[1, 0],[1, 1]], dtype=np.float32)\n",
    "Y_or = np.asarray([0, 1, 1, 1], dtype=np.float32)[:,None]\n",
    "## Let's display our set of input output pairs\n",
    "from tabulate import tabulate\n",
    "print tabulate(np.concatenate((X,Y_or), axis=1), headers=['X1','X2', 'X1 OR X2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X1    X2       F(X)\n",
      "----  ----  ---------\n",
      "   0     0  0.0101972\n",
      "   0     1  0.995924\n",
      "   1     0  0.995924\n",
      "   1     1  1\n"
     ]
    }
   ],
   "source": [
    "## Instantiate a tuneble function \n",
    "F = GenFunc(2, 1)\n",
    "## call the tuning routine  within a TF session\n",
    "with tf.Session() as sess:\n",
    "    learnProgram(sess, F, X, Y_or)\n",
    "    ## display the outputs of the function after tuning\n",
    "    Z = F(X).eval()\n",
    "    print tabulate(np.concatenate((X,Z), axis=1), headers=['X1','X2', 'F(X)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The AND function\n",
    "The data for the AND function is given by the following truth table:\n",
    "\n",
    "| X1 | X2 | X1 AND X2 |\n",
    "|----|:--:|:---------:|\n",
    "| 0  | 0  |    0      |\n",
    "| 0  | 1  |    0      |\n",
    "| 1  | 0  |    0      |\n",
    "| 1  | 1  |    1      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X1    X2        F(X)\n",
      "----  ----  ----------\n",
      "   0     0  1.5132e-06\n",
      "   0     1  0.010144\n",
      "   1     0  0.010144\n",
      "   1     1  0.985796\n"
     ]
    }
   ],
   "source": [
    "Y_and = np.asarray([0, 0, 0, 1], dtype=np.float32)[:,None]\n",
    "F = GenFunc(2, 1)\n",
    "with tf.Session() as sess:\n",
    "    learnProgram(sess, F, X, Y_and)\n",
    "    Z = F(X).eval()\n",
    "    print tabulate(np.concatenate((X,Z), axis=1), headers=['X1','X2', 'F(X)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The XOR function\n",
    "The data for the XOR function is given by the following truth table:\n",
    "\n",
    "| X1 | X2 | X1 XOR X2 |\n",
    "|----|:--:|:---------:|\n",
    "| 0  | 0  |    0      |\n",
    "| 0  | 1  |    1      |\n",
    "| 1  | 0  |    1      |\n",
    "| 1  | 1  |    0      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  X1    X2    F(X)\n",
      "----  ----  ------\n",
      "   0     0     0.5\n",
      "   0     1     0.5\n",
      "   1     0     0.5\n",
      "   1     1     0.5\n"
     ]
    }
   ],
   "source": [
    "Y_xor = np.asarray([0, 1, 1, 0], dtype=np.float32)[:,None]\n",
    "F = GenFunc(2, 1)\n",
    "with tf.Session() as sess:\n",
    "    learnProgram(sess, F, X, Y_xor)\n",
    "    Z = F(X).eval()\n",
    "    print tabulate(np.concatenate((X,Z), axis=1), headers=['X1','X2', 'F(X)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "Note that for the XOR the function tuning did not succed in replicating the input output relation.\n",
    "It turns out that our tunable function can only represent a subset of all binary functions. \n",
    "### Composing tunable modules to increase the expresivity\n",
    "Fortunately, our tunable function does represent the basic Boolean functions. Compositions of basic Boolean functions can give us all Boolean functions from 2 inputs to 1 output.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tuned composition\n",
      "  X1    X2      G(X)\n",
      "----  ----  --------\n",
      "   0     0  0.500242\n",
      "   0     1  0.499136\n",
      "   1     0  0.500858\n",
      "   1     1  0.49975\n",
      "\n",
      " Tuned input module F1\n",
      "  X1    X2     F1(X)\n",
      "----  ----  --------\n",
      "   0     0  0.48937\n",
      "   0     1  0.45656\n",
      "   1     0  0.508306\n",
      "   1     1  0.475408\n",
      "\n",
      " Tuned input module F2\n",
      "  X1    X2     F2(X)\n",
      "----  ----  --------\n",
      "   0     0  0.499844\n",
      "   0     1  0.493648\n",
      "   1     0  0.526637\n",
      "   1     1  0.520456\n",
      "\n",
      " Tuned intermediate module F3\n",
      "  Z1    Z2     F3(Z)\n",
      "----  ----  --------\n",
      "   0     0  0.484133\n",
      "   0     1  0.483214\n",
      "   1     0  0.517987\n",
      "   1     1  0.517069\n"
     ]
    }
   ],
   "source": [
    "F1 = GenFunc(2, 1)\n",
    "F2 = GenFunc(2, 1)\n",
    "F3 = GenFunc(2, 1)\n",
    "## Define a composition of tunable functions\n",
    "def G(X, is_logit=False):\n",
    "    Z1 = F1(X)\n",
    "    Z2 = F2(X)\n",
    "    Z3 = F3(tf.concat((Z1, Z2), axis=1), is_logit=is_logit)\n",
    "    return Z3\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    learnProgram(sess, G, X, Y_xor)\n",
    "    ## Show the tuned composition of functions G(X)\n",
    "    Z = G(X).eval()\n",
    "    print(\"\\n Tuned composition\")\n",
    "    print tabulate(np.concatenate((X,Z), axis=1), headers=['X1','X2', 'G(X)'])\n",
    "    ## Show the tuned module F1(X)\n",
    "    Z1 = F1(X).eval()\n",
    "    print(\"\\n Tuned input module F1\")\n",
    "    print tabulate(np.concatenate((X,Z1), axis=1), headers=['X1','X2', 'F1(X)'])\n",
    "    ## Show the tuned module F2(X)\n",
    "    Z2 = F2(X).eval()\n",
    "    print(\"\\n Tuned input module F2\")\n",
    "    print tabulate(np.concatenate((X,Z2), axis=1), headers=['X1','X2', 'F2(X)'])\n",
    "    ## Show the intermediate tuned module F3(X)\n",
    "    print(\"\\n Tuned intermediate module F3\")\n",
    "    Z3 = F3(X).eval()\n",
    "    print tabulate(np.concatenate((X,Z3), axis=1), headers=['Z1','Z2', 'F3(Z)'])\n",
    "   \n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
